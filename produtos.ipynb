{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoria\n",
      "brinquedo    1020\n",
      "game         1020\n",
      "livro        1020\n",
      "maquiagem    1020\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "products_data = pd.read_csv('produtos.csv', delimiter=';')\n",
    "print(products_data.groupby('categoria').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nome</th>\n",
       "      <th>descricao</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O Hobbit - 7ª Ed. 2013</td>\n",
       "      <td>Produto NovoBilbo Bolseiro é um hobbit que lev...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Livro - It A Coisa - Stephen King</td>\n",
       "      <td>Produto NovoDurante as férias escolares de 195...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...</td>\n",
       "      <td>Produto NovoTodo o reino de Westeros ao alcanc...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Box Harry Potter</td>\n",
       "      <td>Produto Novo e Físico  A série Harry Potter ch...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Livro Origem - Dan Brown</td>\n",
       "      <td>Produto NovoDe Onde Viemos? Para Onde Vamos? R...</td>\n",
       "      <td>livro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                nome  \\\n",
       "0                            O Hobbit - 7ª Ed. 2013    \n",
       "1                 Livro - It A Coisa - Stephen King    \n",
       "2   Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...   \n",
       "3                                  Box Harry Potter    \n",
       "4                          Livro Origem - Dan Brown    \n",
       "\n",
       "                                           descricao categoria  \n",
       "0  Produto NovoBilbo Bolseiro é um hobbit que lev...     livro  \n",
       "1  Produto NovoDurante as férias escolares de 195...     livro  \n",
       "2  Produto NovoTodo o reino de Westeros ao alcanc...     livro  \n",
       "3  Produto Novo e Físico  A série Harry Potter ch...     livro  \n",
       "4  Produto NovoDe Onde Viemos? Para Onde Vamos? R...     livro  "
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  categoria\n",
      "0     livro\n",
      "1     livro\n",
      "2     livro\n",
      "3     livro\n",
      "4     livro\n",
      "                                          informacao\n",
      "0   O Hobbit - 7ª Ed. 2013 Produto NovoBilbo Bols...\n",
      "1   Livro - It A Coisa - Stephen King Produto Nov...\n",
      "2   Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...\n",
      "3   Box Harry Potter Produto Novo e Físico  A sér...\n",
      "4   Livro Origem - Dan Brown Produto NovoDe Onde ...\n"
     ]
    }
   ],
   "source": [
    "# concatenando as colunas nome e descricao\n",
    "products_data['informacao'] = products_data['nome'] + products_data['descricao']\n",
    "\n",
    "# excluindo linhas com valor de informacao ou categoria NaN\n",
    "products_data.dropna(subset=['informacao', 'categoria'], inplace=True)\n",
    "#products_data.drop(columns = '')\n",
    "y = products_data.drop(columns=['nome', 'descricao', 'informacao'])\n",
    "X = products_data.drop(columns=['nome', 'descricao', 'categoria'])\n",
    "\n",
    "print(y.head())\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29% do dataframe foi removido, porém não é possível utilizar detalhes sem classificação ou classificação sem detalhes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoria\n",
      "brinquedo    668\n",
      "game         622\n",
      "livro        838\n",
      "maquiagem    788\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(products_data.groupby('categoria').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(products_data['categoria'],label=\"Count\")\n",
    "# não esta balanceado mas pode ser melhorado posteriormente.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    hobbit - 7ª ed. 2013 produto novobilbo bolseir...\n",
      "1    livro - it coisa - stephen king produto novodu...\n",
      "2    box crônicas gelo fogo pocket 5 livros produto...\n",
      "3    box harry potter produto novo físico série har...\n",
      "4    livro origem - dan brown produto novode onde v...\n",
      "Name: sem_stopwords, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "# transforma a string em caixa baixa e remove stopwords\n",
    "X['sem_stopwords'] = X['informacao'].str.lower().apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "print(X['sem_stopwords'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Não é necessário fazer separação visto que count vectorize já faz isso posteriormente\n",
    "porém é válida para visualização da ocorrência de cada palavra (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [hobbit, 7ª, ed, 2013, produto, novobilbo, bol...\n",
      "1    [livro, it, coisa, stephen, king, produto, nov...\n",
      "2    [box, crônicas, gelo, fogo, pocket, 5, livros,...\n",
      "3    [box, harry, potter, produto, novo, físico, sé...\n",
      "4    [livro, origem, dan, brown, produto, novode, o...\n",
      "Name: tokens, dtype: object\n",
      "0    {'hobbit': 2, '7ª': 1, 'ed': 1, '2013': 2, 'pr...\n",
      "1    {'livro': 1, 'it': 2, 'coisa': 5, 'stephen': 2...\n",
      "2    {'box': 2, 'crônicas': 3, 'gelo': 3, 'fogo': 3...\n",
      "3    {'box': 2, 'harry': 26, 'potter': 21, 'produto...\n",
      "4    {'livro': 1, 'origem': 2, 'dan': 1, 'brown': 2...\n",
      "Name: tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "X['tokens'] = X['sem_stopwords'].apply(tokenizer.tokenize) # aplica o regex tokenizer\n",
    "print(X['tokens'].head())\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = X['tokens'].apply(FreqDist) # calcula a frequência de cada token\n",
    "print(fdist.head()) # frequencia na coluna sem stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A impressão acima mostra a frequência das palavras após tratamentos para lowercase e sem stopwords. Abaixo segue as listas sem tratamentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {'O': 1, 'Hobbit': 1, '7ª': 1, 'Ed': 1, '2013'...\n",
      "1    {'Livro': 1, 'It': 2, 'A': 3, 'Coisa': 5, 'Ste...\n",
      "2    {'Box': 2, 'As': 1, 'Crônicas': 3, 'De': 1, 'G...\n",
      "3    {'Box': 1, 'Harry': 26, 'Potter': 21, 'Produto...\n",
      "4    {'Livro': 1, 'Origem': 2, 'Dan': 1, 'Brown': 2...\n",
      "Name: informacao, dtype: object\n"
     ]
    }
   ],
   "source": [
    "fdist_comstop = X['informacao'].apply(tokenizer.tokenize).apply(FreqDist)\n",
    "print(fdist_comstop.head()) # Frequencia na coluna com stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informacao</th>\n",
       "      <th>sem_stopwords</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O Hobbit - 7ª Ed. 2013 Produto NovoBilbo Bols...</td>\n",
       "      <td>hobbit - 7ª ed. 2013 produto novobilbo bolseir...</td>\n",
       "      <td>[hobbit, 7ª, ed, 2013, produto, novobilbo, bol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Livro - It A Coisa - Stephen King Produto Nov...</td>\n",
       "      <td>livro - it coisa - stephen king produto novodu...</td>\n",
       "      <td>[livro, it, coisa, stephen, king, produto, nov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...</td>\n",
       "      <td>box crônicas gelo fogo pocket 5 livros produto...</td>\n",
       "      <td>[box, crônicas, gelo, fogo, pocket, 5, livros,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Box Harry Potter Produto Novo e Físico  A sér...</td>\n",
       "      <td>box harry potter produto novo físico série har...</td>\n",
       "      <td>[box, harry, potter, produto, novo, físico, sé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Livro Origem - Dan Brown Produto NovoDe Onde ...</td>\n",
       "      <td>livro origem - dan brown produto novode onde v...</td>\n",
       "      <td>[livro, origem, dan, brown, produto, novode, o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          informacao  \\\n",
       "0   O Hobbit - 7ª Ed. 2013 Produto NovoBilbo Bols...   \n",
       "1   Livro - It A Coisa - Stephen King Produto Nov...   \n",
       "2   Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...   \n",
       "3   Box Harry Potter Produto Novo e Físico  A sér...   \n",
       "4   Livro Origem - Dan Brown Produto NovoDe Onde ...   \n",
       "\n",
       "                                       sem_stopwords  \\\n",
       "0  hobbit - 7ª ed. 2013 produto novobilbo bolseir...   \n",
       "1  livro - it coisa - stephen king produto novodu...   \n",
       "2  box crônicas gelo fogo pocket 5 livros produto...   \n",
       "3  box harry potter produto novo físico série har...   \n",
       "4  livro origem - dan brown produto novode onde v...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [hobbit, 7ª, ed, 2013, produto, novobilbo, bol...  \n",
       "1  [livro, it, coisa, stephen, king, produto, nov...  \n",
       "2  [box, crônicas, gelo, fogo, pocket, 5, livros,...  \n",
       "3  [box, harry, potter, produto, novo, físico, sé...  \n",
       "4  [livro, origem, dan, brown, produto, novode, o...  "
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer.\n",
    "Extrai o sufixo das palavras, usado para facilitar a associação entre palavras com sentidos próximos.\n",
    "\n",
    "\n",
    "Precisão razoável.\n",
    "\n",
    "\n",
    "Por exemplo:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "livr\n",
      "livreir\n",
      "livr\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"livro\"))\n",
    "print(stemmer.stem(\"livreiro\"))\n",
    "print(stemmer.stem(\"livraria\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['stemmed'] = X['tokens'].apply(lambda x: [stemmer.stem(y) for y in x]) # Aplica o stemmer pra cada palavra\n",
    "X = X.drop(columns=['sem_stopwords']) # Exclui a coluna sem o stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>informacao</th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O Hobbit - 7ª Ed. 2013 Produto NovoBilbo Bols...</td>\n",
       "      <td>[hobbit, 7ª, ed, 2013, produto, novobilbo, bol...</td>\n",
       "      <td>[hobbit, 7ª, ed, 2013, produt, novobilb, bolse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Livro - It A Coisa - Stephen King Produto Nov...</td>\n",
       "      <td>[livro, it, coisa, stephen, king, produto, nov...</td>\n",
       "      <td>[livr, it, cois, stephen, king, produt, novodu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...</td>\n",
       "      <td>[box, crônicas, gelo, fogo, pocket, 5, livros,...</td>\n",
       "      <td>[box, crônic, gel, fog, pocket, 5, livr, produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Box Harry Potter Produto Novo e Físico  A sér...</td>\n",
       "      <td>[box, harry, potter, produto, novo, físico, sé...</td>\n",
       "      <td>[box, harry, pott, produt, nov, físic, séri, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Livro Origem - Dan Brown Produto NovoDe Onde ...</td>\n",
       "      <td>[livro, origem, dan, brown, produto, novode, o...</td>\n",
       "      <td>[livr, orig, dan, brown, produt, novod, onde, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          informacao  \\\n",
       "0   O Hobbit - 7ª Ed. 2013 Produto NovoBilbo Bols...   \n",
       "1   Livro - It A Coisa - Stephen King Produto Nov...   \n",
       "2   Box  As Crônicas De Gelo E Fogo  Pocket  5 Li...   \n",
       "3   Box Harry Potter Produto Novo e Físico  A sér...   \n",
       "4   Livro Origem - Dan Brown Produto NovoDe Onde ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [hobbit, 7ª, ed, 2013, produto, novobilbo, bol...   \n",
       "1  [livro, it, coisa, stephen, king, produto, nov...   \n",
       "2  [box, crônicas, gelo, fogo, pocket, 5, livros,...   \n",
       "3  [box, harry, potter, produto, novo, físico, sé...   \n",
       "4  [livro, origem, dan, brown, produto, novode, o...   \n",
       "\n",
       "                                             stemmed  \n",
       "0  [hobbit, 7ª, ed, 2013, produt, novobilb, bolse...  \n",
       "1  [livr, it, cois, stephen, king, produt, novodu...  \n",
       "2  [box, crônic, gel, fog, pocket, 5, livr, produ...  \n",
       "3  [box, harry, pott, produt, nov, físic, séri, h...  \n",
       "4  [livr, orig, dan, brown, produt, novod, onde, ...  "
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head() # Mostra o dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['informacao']) # Exclui as colunas antigas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### passo opcional: \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "X['stemmed'] = X['stemmed'].apply(lambda col: le.fit_transform(col)) # transforma as palavras em inteiros\n",
    "print(X['stemmed'].head(10))\n",
    "\n",
    "y = le.fit_transform(y) # transforma cada categoria em um inteiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {'hobbit': 2, '7ª': 1, 'ed': 1, '2013': 2, 'pr...\n",
      "1    {'livr': 1, 'it': 2, 'cois': 5, 'stephen': 2, ...\n",
      "2    {'box': 2, 'crônic': 3, 'gel': 3, 'fog': 3, 'p...\n",
      "3    {'box': 2, 'harry': 26, 'pott': 21, 'produt': ...\n",
      "4    {'livr': 1, 'orig': 2, 'dan': 1, 'brown': 2, '...\n",
      "5    {'escur': 2, 'cinquent': 1, 'tons': 1, 'olhos'...\n",
      "6    {'silmarillion': 2, '5ª': 1, 'ed': 1, '2011': ...\n",
      "7    {'pequen': 2, 'princip': 1, 'príncip': 1, 'per...\n",
      "8    {'ed': 6, 'lorrain': 6, 'warren': 6, 'demonolo...\n",
      "9    {'box': 2, 'franz': 2, 'kafk': 5, '1883': 1, '...\n",
      "Name: stemmed, dtype: object\n"
     ]
    }
   ],
   "source": [
    "fdist = X['stemmed'].apply(FreqDist) # calcula a frequência de cada token\n",
    "print(fdist.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>strings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[hobbit, 7ª, ed, 2013, produto, novobilbo, bol...</td>\n",
       "      <td>[hobbit, 7ª, ed, 2013, produt, novobilb, bolse...</td>\n",
       "      <td>hobbit 7ª ed 2013 produt novobilb bolseir hobb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[livro, it, coisa, stephen, king, produto, nov...</td>\n",
       "      <td>[livr, it, cois, stephen, king, produt, novodu...</td>\n",
       "      <td>livr it cois stephen king produt novodur fér e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[box, crônicas, gelo, fogo, pocket, 5, livros,...</td>\n",
       "      <td>[box, crônic, gel, fog, pocket, 5, livr, produ...</td>\n",
       "      <td>box crônic gel fog pocket 5 livr produt novoto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[box, harry, potter, produto, novo, físico, sé...</td>\n",
       "      <td>[box, harry, pott, produt, nov, físic, séri, h...</td>\n",
       "      <td>box harry pott produt nov físic séri harry pot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[livro, origem, dan, brown, produto, novode, o...</td>\n",
       "      <td>[livr, orig, dan, brown, produt, novod, onde, ...</td>\n",
       "      <td>livr orig dan brown produt novod onde viem ond...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [hobbit, 7ª, ed, 2013, produto, novobilbo, bol...   \n",
       "1  [livro, it, coisa, stephen, king, produto, nov...   \n",
       "2  [box, crônicas, gelo, fogo, pocket, 5, livros,...   \n",
       "3  [box, harry, potter, produto, novo, físico, sé...   \n",
       "4  [livro, origem, dan, brown, produto, novode, o...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [hobbit, 7ª, ed, 2013, produt, novobilb, bolse...   \n",
       "1  [livr, it, cois, stephen, king, produt, novodu...   \n",
       "2  [box, crônic, gel, fog, pocket, 5, livr, produ...   \n",
       "3  [box, harry, pott, produt, nov, físic, séri, h...   \n",
       "4  [livr, orig, dan, brown, produt, novod, onde, ...   \n",
       "\n",
       "                                             strings  \n",
       "0  hobbit 7ª ed 2013 produt novobilb bolseir hobb...  \n",
       "1  livr it cois stephen king produt novodur fér e...  \n",
       "2  box crônic gel fog pocket 5 livr produt novoto...  \n",
       "3  box harry pott produt nov físic séri harry pot...  \n",
       "4  livr orig dan brown produt novod onde viem ond...  "
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"strings\"]= X[\"stemmed\"].str.join(\" \") # reunindo cada elemento da lista\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build three predictive models: 1) Decision Tree, 2) Random Forest, and 3) Gradient Boost;\n",
    "assess the predictive power of these models and select the most superior one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando os dados em treino e teste\n",
    "80% para treino e 20% para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    X[\"strings\"], \n",
    "    y[\"categoria\"], \n",
    "    test_size = 0.2, \n",
    "    random_state = 10\n",
    ")\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(X[\"strings\"])\n",
    "\n",
    "marvin_dataset = {\n",
    "    \"X_train\": vect.transform(X_train),\n",
    "    \"X_test\": vect.transform(X_test),\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\" : y_test,\n",
    "    \"vect\": vect\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'csr_matrix'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-578-4f9abb29145c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m dataset = {\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"X_train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1288\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    757\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'M8[ns]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130     voc alasc ediça comemor produt nov físic voc a...\n",
       "3645    cartã 41 700 cash warfac envi imediat level up...\n",
       "2132    batom líqu matt ricost 1 unidad cor nov disp 1...\n",
       "3091    3 000 gold nemes allianc hord wow legion nemes...\n",
       "3762    the king of fighters xiv 14 ps4 míd físic nov ...\n",
       "3781    monst hunt world xbox one original onlin digit...\n",
       "1581    4 bonec lig justic batman superman lantern fla...\n",
       "3243    the last guardian ps4 míid físic nov português...\n",
       "86      1984 produt novoa lad a revolu bich livr 1984 ...\n",
       "3601    crash bandicoot 1 2 3 trilog ps3 digital jog h...\n",
       "3111    sonic generations míd físic lacr xbox 360 dife...\n",
       "656     dillon pin extrator decapping pin decapping ex...\n",
       "972     livr it cois cuj cap dur ilumin cap dur livr s...\n",
       "232     senhor mosc produt novopublic original 1954 se...\n",
       "1473    coleçã kit 18 miniatur sup mari bros pront ent...\n",
       "1494    dragon ball z gt dragã ball 20 bonec anim goku...\n",
       "1828    peluc pikachu pokebol pokemons go 23cm a084 pe...\n",
       "997     tartarug lá embaix produt novodepo seis anos m...\n",
       "1846    sup wings articul grand brinqued bonec valor r...\n",
       "2484    tintur gel tatuag sobrancelh prov d agu delin ...\n",
       "3234    cartã battl net r 60 rea blizzard wow world of...\n",
       "2990    malet maquiag complet original boticári mak b ...\n",
       "3655    jog uncharted 4 thiefs end ps4 míd físic descr...\n",
       "1050    varinh harry pott acompanh caixinh valor unitá...\n",
       "366     lug nenhum produt novoprimeir romanc neil gaim...\n",
       "2451    malet maquiag infantil princes antialérg brind...\n",
       "2261    lâmin tebor 100 unidad fret grat pioneir ram v...\n",
       "838     guerr luz cicl trev vol 3 produt nov a guerr l...\n",
       "383     destru diári qualqu lug produt novodepo entret...\n",
       "1756    dragon ball goku gohan trunks veget cell freez...\n",
       "                              ...                        \n",
       "1870    kit blaz and the monst machin kit 6 carrinh fr...\n",
       "479     senhor ané trilog volum únic cap film produt n...\n",
       "3890    patch bmpes 2 1 pes 2018 dlc 3 0 pc atualiz da...\n",
       "14      gravity falls journal 3 produt nov físic journ...\n",
       "83      escur grey vol 2 produt novo l jam revisit cin...\n",
       "2324    malet 3 andar maquiag esmalt rodinh extra gran...\n",
       "1204    dinossaur eletrôn 3d anda acend olhos dinossau...\n",
       "1582    roblox 100 original escolh model personag 1 vi...\n",
       "679     box hqs crônic gel fog gam of thron 4 volum bo...\n",
       "2823    malet maquiag acessóri kit c 3 malet kit malet...\n",
       "3318    2 000 gold wow gold our azralon nemes 2 0k bon...\n",
       "272     amityvill produt novodepo pass algum déc fech ...\n",
       "414     juliet produt novo esper sequênc rom traz volt...\n",
       "2104    lágrim unicórni 1 unidad primerembelezadoról i...\n",
       "3269    football manag 2018 ste original touch 2018 va...\n",
       "3494    gta san andre hd remast ps3 psn envi agor jog ...\n",
       "43      harry pott pedr filosofal cap dur produt nov f...\n",
       "3472    dark souls 1 ps3 nov míd físic lacr dark souls...\n",
       "3075    call of duty wwi ww2 ps4 míd físic lacr portug...\n",
       "1382    than marvel select comics dc vingador iron man...\n",
       "1282    estátu hawkey dioram 1 6 avengers age of ultro...\n",
       "2744    palet sombr max lov 9 cor kit maquiag matt lem...\n",
       "2628    cíli crème original 100 cabel human 747 m 1 pa...\n",
       "2064    kit palet ruby ros sobrancelh sombr corret pal...\n",
       "869     kit livr séri crossfir 5 livr séri crossfir 5 ...\n",
       "2701    miss ros palet ilumin glow kit 024 n3 ilumin g...\n",
       "1522    maquin bolinh 250 und bolinh pul pul 27mm melh...\n",
       "1768    bonec goku ssj4 dragonball gt sup z kai nov ba...\n",
       "623     marilyn monro produt novoel quer ser admir dis...\n",
       "1675    bonec hom aranh gigant 45 cm mim bonec hom ara...\n",
       "Name: strings, Length: 2332, dtype: object"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means, support vector machine, bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
